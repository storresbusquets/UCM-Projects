{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "376a320b672c7b44e2effcf32effc572", "grade": false, "grade_id": "cell-f8987996be9f1238", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# Utilizaci\u00f3n del servicio de alquiler de bicicletas en Toronto en el a\u00f1o 2018\n\n### Disponible en Kaggle en el siguiente enlace (que NO debe usarse para el ejercicio, sino los CSV que se adjuntaron al email):\nhttps://www.kaggle.com/jackywang529/toronto-bikeshare-data\n\n\nEl prop\u00f3sito de este an\u00e1lisis es utilizar los conjuntos de datos trimestrales del a\u00f1o 2018 de la empresa de alquiler de bicicletas en Toronto. Se trata de *cuatro* conjuntos de datos separados, que incluyen entre 178.559 y 822.536 observaciones, siempre con nueve variables. Cada fila representa un viaje realizado."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "9a6b4dc108ddf890c659e33701965428", "grade": false, "grade_id": "cell-f74d7bfd01811789", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Variables y significado"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "0cb790eed3719dc8d6cfd639c9176b4a", "grade": false, "grade_id": "cell-9cfb34982bd4eb04", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Las variables utilizadas para describir cada viaje son:\n\n* trip_id \u2013 identificador global del viaje\n* trip_duration_seconds \u2013 duraci\u00f3n del viaje en segundos\n* from_station_id \u2013 identificador num\u00e9rico de la estaci\u00f3n de origen\n* trip_start_time \u2013 instante (timestamp) en el que se inici\u00f3 el viaje\n* from_station_name \u2013 nombre de la intersecci\u00f3n m\u00e1s cercana a la estaci\u00f3n origen\n* trip_stop_time \u2013 instante (timestamp) en el que finaliz\u00f3 el viaje\n* to_station_id \u2013 identificador num\u00e9rico de la estaci\u00f3n de destino\n* to_station_name \u2013 nombre de la intersecci\u00f3n m\u00e1s cercana a la estaci\u00f3n de destino\n* user_type \u2013 tipo de usuario (indicador binario): miembro registrado con cuota anual / usuario ocasional no registrado"}, {"cell_type": "markdown", "metadata": {}, "source": "**Nombre completo del alumno:**  Santiago Torres Busquets"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "76dc5b331cac3113e9e77522358617bf", "grade": false, "grade_id": "cell-b4f9c37a2b92d2e6", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**INSTRUCCIONES**: en cada celda debes responder a la pregunta formulada, asegur\u00e1ndote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del c\u00e1lculo quede guardado exactamente en la variable que ven\u00eda inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). **No olvides borrar la l\u00ednea *raise NotImplementedError()* de cada celda cuando hayas completado la soluci\u00f3n de esa celda y quieras probarla**.\n\nDespu\u00e9s de cada celda evaluable ver\u00e1s una celda con c\u00f3digo. Ejec\u00fatala (no modifiques su c\u00f3digo) y te dir\u00e1 si tu soluci\u00f3n es correcta o no. En caso de ser correcta, se ejecutar\u00e1 correctamente y no mostrar\u00e1 nada, pero si no lo es mostrar\u00e1 un error. Adem\u00e1s de esas pruebas, se realizar\u00e1n algunas m\u00e1s (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la soluci\u00f3n correcta o no. Aseg\u00farate de que, al menos, todas las celdas indican que el c\u00f3digo es correcto antes de enviar el notebook terminado."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "8bae739df33929bae8d756987e80caf8", "grade": false, "grade_id": "cell-69ec0993eeaff3ac", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Sobre los cuatro datasets anteriores (Bike Share Toronto Ridership_Q1 2018.csv hasta Q4) se pide:"}, {"cell_type": "markdown", "metadata": {}, "source": "**(1 punto)** Ejercicio 1\n\n* Leer por separado cada uno de ellos (sin cachear), tratando de que Spark infiera el tipo de dato de cada columna, y **unirlos en un solo DF** que tampoco debe ser cacheada todav\u00eda, ya que en el siguiente paso a\u00fan realizaremos otro pre-procesamiento.\n* Los cuatro contienen las mismas columnas por lo que no habr\u00e1 problemas para utilizar la operaci\u00f3n `union` encadenada tres veces para crear el DF final."}, {"cell_type": "code", "execution_count": 1, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "44717972bf7cac300a5ad876d9fd6632", "grade": false, "grade_id": "read_csv", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\ntripsQ1 = spark.read.option(\"header\",\"true\").option(\"inferSchema\", \"true\").csv(\"gs://masterucm-hadoopspark-stb/datos/Bike Share Toronto Ridership_Q1 2018.csv\") # Primer CSV\ntripsQ2 = spark.read.option('header','true').option(\"inferSchema\", \"true\").csv(\"gs://masterucm-hadoopspark-stb/datos/Bike Share Toronto Ridership_Q2 2018.csv\") # Segundo CSV\ntripsQ3 = spark.read.option('header','true').option(\"inferSchema\", \"true\").csv(\"gs://masterucm-hadoopspark-stb/datos/Bike Share Toronto Ridership_Q3 2018.csv\") # Tercer CSV\ntripsQ4 = spark.read.option('header','true').option(\"inferSchema\", \"true\").csv(\"gs://masterucm-hadoopspark-stb/datos/Bike Share Toronto Ridership_Q4 2018.csv\") # Cuarto CSV\ntripsTorontoRawDF = tripsQ1.union(tripsQ2)\\\n                           .union(tripsQ3)\\\n                           .union(tripsQ4)# Uni\u00f3n de todos\n\n# YOUR CODE HERE\n# raise NotImplementedError"}, {"cell_type": "code", "execution_count": 2, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "5e6fa646bfe97c4d7d321099133d99e4", "grade": true, "grade_id": "read_csv_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.types import DoubleType\nassert(tripsTorontoRawDF.count() == 1922955)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "6eb33253c64dbd7870725e3e6d6a8e0f", "grade": false, "grade_id": "cell-b90f5b934eda250e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(1 punto)** Ejercicio 2\n\n* Las columnas `trip_start_time` y `trip_stop_time` son en realidad instantes de tiempo que Spark deber\u00eda procesar como timestamp. Reemplaza **ambas columnas** por su versi\u00f3n convertida a timestamp, utilizando `withColumn` y donde el nuevo valor de la columna viene dado por el siguiente c\u00f3digo:\n        F.from_unixtime(F.unix_timestamp('nombreColumna', 'M/d/yyyy H:mm')).cast(\"timestamp\"))\nEl DF resultante debe ser almacenado en la variable `tripsTorontoDF`.\n"}, {"cell_type": "code", "execution_count": 3, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "9129c1d06eef70a5b6922585902dfa36", "grade": false, "grade_id": "convert_timestamp", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# No olvides los imports que necesites...\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n# L\u00cdNEAS EVALUABLES, NO RENOMBRAR LAS VARIABLES\ntripsTorontoDF = tripsTorontoRawDF.withColumn(\"trip_start_time\",F.from_unixtime(F.unix_timestamp('trip_start_time', 'M/d/yyyy H:mm')).cast(\"timestamp\"))\\\n                                  .withColumn(\"trip_stop_time\",F.from_unixtime(F.unix_timestamp('trip_stop_time', 'M/d/yyyy H:mm')).cast(\"timestamp\"))\n# YOUR CODE HERE\n# raise NotImplementedError"}, {"cell_type": "code", "execution_count": 4, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "3ebd685fd8c8fcd5062ecd1c29adcd4b", "grade": true, "grade_id": "convert_timestamp_tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "typesDict = dict(tripsTorontoDF.dtypes)\nassert(typesDict[\"trip_start_time\"] == \"timestamp\") \nassert(typesDict[\"trip_stop_time\"] == \"timestamp\") "}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "5d11d72889323abc3fa6626ed1da257f", "grade": false, "grade_id": "cell-fc88821f19453a51", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(1 punto)** Ejercicio 3\n\nPartiendo de `tripsTorontoDF`, realizar las siguientes transformaciones encadenadas en este orden para crear un nuevo DF:\n* Primero, debemos quedarnos solamente con las filas donde `trip_start_time` no sea null.\n* Sobre el DF resultado de lo anterior, a\u00f1adir una columna adicional **Mes** y con el mes representado en **trip_start_time**. Dicha columna ser\u00e1 de tipo entero y se puede obtener usando `withColumn` con la funci\u00f3n `F.month(\"colName\")`, que recibe un nombre de columna y devuelve un objeto columna de enteros que van de 1 a 12. \n* Encadenar esta transformaci\u00f3n con otra en la que la columna **Mes** sea reemplazada por su traducci\u00f3n a  cadena de caracteres de 3 letras, siendo la correspondencia 1: Ene, 2: Feb, 3: Mar, 4: Abr, 5: May, 6: Jun, 7: Jul, 8: Ago, 9: Sep, 10: Oct, 11: Nov, 12: Dic.\n* Finalmente, a\u00f1adir una nueva columna **Hora** que contenga la hora de inicio del viaje, aplicando `withColumn` con la funci\u00f3n `F.hour(\"colName\")` que recibe un nombre de columna y recibe un objeto columna de enteros de 0 a 23.\n* El DF resultante de todas estas transformaciones debe guardarse en la variable `tripsTorontoTimesDF`, que por tanto tendr\u00e1 2 columnas m\u00e1s que el DF original `tripsTorontoDF`, y que debe quedar **cacheado**."}, {"cell_type": "code", "execution_count": 5, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "2aa969fc933e984b995caf9a857feede", "grade": false, "grade_id": "renombrar_mes_hora", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR VARIABLES\n# imports......\nfrom pyspark.sql import functions as F\n\ntripsTorontoTimesDF = tripsTorontoDF.filter(F.col(\"trip_start_time\").isNotNull())\\\n                                    .withColumn(\"Mes\", F.month(\"trip_start_time\"))\\\n                                    .withColumn(\"Mes\", \n                                                 F.when(F.col(\"Mes\") == 1,\"Ene\")\\\n                                                  .when(F.col(\"Mes\") == 2,\"Feb\")\\\n                                                  .when(F.col(\"Mes\") == 3,\"Mar\")\\\n                                                  .when(F.col(\"Mes\") == 4,\"Abr\")\\\n                                                  .when(F.col(\"Mes\") == 5,\"May\")\\\n                                                  .when(F.col(\"Mes\") == 6,\"Jun\")\\\n                                                  .when(F.col(\"Mes\") == 7,\"Jul\")\\\n                                                  .when(F.col(\"Mes\") == 8,\"Ago\")\\\n                                                  .when(F.col(\"Mes\") == 9,\"Sep\")\\\n                                                  .when(F.col(\"Mes\") == 10,\"Oct\")\\\n                                                  .when(F.col(\"Mes\") == 11,\"Nov\")\\\n                                                  .otherwise(\"Dic\"))\\\n                                    .withColumn(\"Hora\", F.hour(\"trip_start_time\"))\\\n                                    .cache() # Cacheo del DataFrame\n# YOUR CODE HERE\n# raise NotImplementedError"}, {"cell_type": "code", "execution_count": 6, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "b60aed54e852d77c7fa8482a4c34379c", "grade": true, "grade_id": "renombrar_mes_hora_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tripsPerMonth = tripsTorontoTimesDF.groupBy(\"Mes\").count().sort(\"Mes\").collect()\nassert(tripsPerMonth[0][\"count\"] == 94783)\nassert(tripsPerMonth[1][\"count\"] == 281219)\nassert(tripsPerMonth[2][\"count\"] == 83324)\nassert(tripsPerMonth[3][\"count\"] == 43859)\nassert(tripsPerMonth[4][\"count\"] == 49731)\nassert(tripsPerMonth[5][\"count\"] == 286316)\nassert(tripsPerMonth[6][\"count\"] == 250837)\nassert((tripsPerMonth[7][\"count\"] == 84959) | (tripsPerMonth[7][\"count\"] == 84969))\nassert(tripsPerMonth[8][\"count\"] == 212750)\nassert(tripsPerMonth[9][\"count\"] == 104287)\nassert(tripsPerMonth[10][\"count\"] == 175879)\nassert(tripsPerMonth[11][\"count\"] == 255001)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "f98c8387ad8a683f0ea2f1c6e441e07a", "grade": false, "grade_id": "cell-a71a6b17b1e0d613", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(1 punto)** Ejercicio 4\n\n* Partiendo de `tripsTorontoTimesDF`, crear un nuevo DataFrame con **tantas filas como horas tiene el d\u00eda, y tantas columnas como meses del a\u00f1o** de manera que cada celda indique el **n\u00famero de viajes** que comenzaron a esa hora en ese mes del a\u00f1o. Guardar el resultado en la variable `tripsPerMonthAndHourDF`, cuyas filas deben quedar ordenadas en base a la hora (de 0 a 23), y cuyas columnas deben estar tambi\u00e9n ordenadas desde `\"Ene\"` a `\"Dic\"`, con `\"Hora\"` como primera columna."}, {"cell_type": "code", "execution_count": 7, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "416095b927a3c16ca9843df1228e43d3", "grade": false, "grade_id": "numero_categorias", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR VARIABLES\ntripsPerMonthAndHourDF = tripsTorontoTimesDF.groupBy(\"Hora\")\\\n                                            .pivot(\"Mes\")\\\n                                            .count()\\\n                                            .sort(\"Hora\")\\\n                                            .select(\"Hora\",\"Ene\",\"Feb\",\"Mar\",\"Abr\",\"May\",\"Jun\",\"Jul\",\"Ago\",\"Sep\",\"Oct\",\"Nov\",\"Dic\")\n# YOUR CODE HERE\n# raise NotImplementedError"}, {"cell_type": "code", "execution_count": 8, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "d64ff89bcbd46871e937ae34db834496", "grade": true, "grade_id": "numero_categorias_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "assert(len(tripsPerMonthAndHourDF.columns) == 13)\nassert(tripsPerMonthAndHourDF.columns[0] == \"Hora\")\nassert(tripsPerMonthAndHourDF.columns[12] == \"Dic\")\nassert(tripsPerMonthAndHourDF.count() == 24)\ntodasHoras = tripsPerMonthAndHourDF.collect()\nassert((todasHoras[0][\"Hora\"] == 0) & (todasHoras[0][\"Dic\"]==782))\nassert((todasHoras[23][\"Hora\"] == 23) & (todasHoras[23][\"Dic\"]==1208))"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "2d2b0a3fb505c65793ee15bf17e87e87", "grade": false, "grade_id": "cell-c5ec05706eccd480", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(3 puntos)** Ejercicio 5. \n\nPartiendo de `tripsTorontoTimesDF` definido anteriormente, a\u00f1adir las siguientes columnas:\n\n* Primero, tres columnas adicionales llamadas `dur_media`, `dur_min`, `dur_max` que contengan, respectivamente, **la duraci\u00f3n media, m\u00ednima y m\u00e1xima de los viajes que parten de esa misma estaci\u00f3n de origen (from_station_id) a esa misma hora y en ese mismo mes del a\u00f1o**. Es decir, queremos una columna extra para que podamos tener, junto a cada viaje, informaci\u00f3n agregada de los viajes similares, entendidos como aquellos que salieron a la misma hora de la misma estaci\u00f3n. **No se debe utilizar JOIN sino solo funciones de ventana**.\n* A continuaci\u00f3n, otra columna adicional `diff_dur_porc` que contenga la diferencia, medida en porcentaje, entre la duraci\u00f3n del viaje y la duraci\u00f3n media de los viajes similares calculada en el apartado anterior. Dicha diferencia debe calcularse como la resta de la duraci\u00f3n del viaje menos la duraci\u00f3n media, dividida entre la duraci\u00f3n media y multiplicada por 100. El resultado debe obtenerse aplicando operaciones aritm\u00e9ticas con columnas existentes, **sin utilizar `when`**.\n* El DF resultante con las 4 columnas nuevas que hemos a\u00f1adido debe almacenarse en la variable `tripsTorontoExtraInfoDF`."}, {"cell_type": "code", "execution_count": 9, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "14773754aebd257287c3ab4a00b00379", "grade": false, "grade_id": "ventana", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR VARIABLES\n# imports necesarios..........\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window\nwindowHoraMesEstacion = Window().partitionBy(\"from_station_id\",\"Hora\",\"Mes\")\n\ntripsTorontoExtraInfoDF = tripsTorontoTimesDF.withColumn(\"dur_media\",\n                                                         F.mean(\"trip_duration_seconds\").over(windowHoraMesEstacion))\\\n                                             .withColumn(\"dur_min\", \n                                                         F.min(\"trip_duration_seconds\").over(windowHoraMesEstacion))\\\n                                             .withColumn(\"dur_max\", \n                                                         F.max(\"trip_duration_seconds\").over(windowHoraMesEstacion))\\\n                                             .withColumn(\"diff_dur_porc\", \n                                                         (F.col(\"trip_duration_seconds\") - F.col(\"dur_media\")) / F.col(\"dur_media\") *100)\n# YOUR CODE HERE\n# raise NotImplementedError"}, {"cell_type": "code", "execution_count": 10, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "729b30b98cb70cb301206a9bce962a58", "grade": true, "grade_id": "ventana_test", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "r = tripsTorontoExtraInfoDF.where(\"trip_id = '2970611'\").head()\nassert(r.dur_media - 783.366666667 < 0.001)\nassert(r.diff_dur_porc - 44.24918088591975 < 0.001)\nassert(r.dur_min == 167)\nassert(r.dur_max == 2333)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "6d197cc479ae5510f149b5430d62a157", "grade": false, "grade_id": "cell-9ebe35c4b4325269", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(3 puntos)** Ejercicio 6\n\n* Partiendo de `tripsTorontoTimesDF`, crear un **grafo** llamado `bikeGraph` utilizando como identificador de los v\u00e9rtices los identificadores de las estaciones. Construye primero un DF con todos los identificadores de las estaciones, simplemente seleccionando **from_station_id**, renombrando adecuadamente el nombre de columna. Puedes almacenar este DF en la variable `verticesDF`. Tambi\u00e9n tendr\u00e1s que renombrar las columnas **from_station_id** y **to_station_id** en el DF de aristas, para el que adem\u00e1s deber\u00e1s seleccionar solo dichas columnas y quitar las filas repetidas ya que solo necesitamos considerar una vez cada ruta (cada pareja de estaci\u00f3n inicial y final). Puedes almacenar el resultado del renombramiento y la eliminaci\u00f3n de repetidos en la variable `edgesDF`.\n* Una vez creado, aplica el algoritmo `pageRank` pasando como \u00fanico par\u00e1metro `maxIter = 5`. El algoritmo puede llegar a emplear m\u00e1s de 10 minutos. \n* Almacena el grafo devuelto por dicha funci\u00f3n en la variable `pageRankGraph`, recupera el DF de sus v\u00e9rtices, ord\u00e9nalo descendentemente en base a la columna `pagerank` y almacena el resultado en la variable `sortedPageRankGraphVerticesDF`\n* Obt\u00e9n el identificador de la estaci\u00f3n m\u00e1s relevante (con mayor valor de la m\u00e9trica pageRank, que ocupar\u00e1 la primera fila tras la ordenaci\u00f3n), y almacena dicho identificador en la variable `id_mas_relevante`.\n* Crea un nuevo DF de una sola fila y tres columnas llamadas `dur_media`, `dur_min` y `dur_max` con la duraci\u00f3n **media, m\u00ednima y m\u00e1xima** de los viajes de `tripsTorontoTimesDF` que **empiezan** en dicha estaci\u00f3n (sin tener en cuenta distinci\u00f3n de horas o meses). **No debe usarse la funci\u00f3n `withColumn` sino crear las columnas al vuelo con `select`**. Debe quedar almacenado en la variable `durEstMasRelevantesDF`"}, {"cell_type": "code", "execution_count": 11, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "fc1f29ab4015788d03e8e01be66ea200", "grade": false, "grade_id": "graph", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/06/05 10:21:10 WARN org.apache.spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\n                                                                                \r"}], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR VARIABLES\n# imports necesarios..........\nfrom pyspark.sql import functions as F\nfrom graphframes import GraphFrame\n# Descomentar la siguiente l\u00ednea antes de lanzar pageRank:\nspark.sparkContext.setCheckpointDir(\"/tmp\")\n\nverticesDF = tripsTorontoTimesDF.select(F.col(\"from_station_id\").alias(\"id\")).distinct()\nedgesDF = tripsTorontoTimesDF.select(F.col(\"from_station_id\").alias('src'),\n                                     F.col(\"to_station_id\").alias('dst')).distinct()\nbikeGraph = GraphFrame(verticesDF, edgesDF)\npageRankGraph = bikeGraph.pageRank(maxIter=5)\nsortedPageRankGraphVerticesDF = pageRankGraph.vertices.orderBy(F.col(\"pagerank\").desc())\nid_mas_relevante = sortedPageRankGraphVerticesDF.first()[0]\ndurEstMasRelevantesDF = tripsTorontoTimesDF.filter(F.col(\"from_station_id\")==id_mas_relevante)\\\n                   .groupBy(\"from_station_id\").agg(F.mean(\"trip_duration_seconds\").alias(\"dur_media\"),\n                                                   F.min(\"trip_duration_seconds\").alias(\"dur_min\"),\n                                                   F.max(\"trip_duration_seconds\").alias(\"dur_max\"))\\\n                   .select(\"dur_media\",\"dur_min\",\"dur_max\")\n# YOUR CODE HERE\n# raise NotImplementedError"}, {"cell_type": "code", "execution_count": 12, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "cce36154f8b55fd019b7f285e1273958", "grade": true, "grade_id": "graph_tests", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1211, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while receiving\n/usr/lib/spark/python/pyspark/context.py:460: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n  warnings.warn(\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:32927)\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n    connection = self.deque.pop()\nIndexError: pop from an empty deque\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n    self.socket.connect((self.address, self.port))\nConnectionRefusedError: [Errno 111] Connection refused\n"}], "source": "assert(sortedPageRankGraphVerticesDF.head()[\"pagerank\"] - 1.4427 < 0.01)\nassert(durEstMasRelevantesDF.count() == 1)\nassert(len(durEstMasRelevantesDF.columns) == 3)\nrEstMasRelevantes = durEstMasRelevantesDF.head()\nassert(rEstMasRelevantes.dur_min == 61)\nassert(id_mas_relevante == 7060)\nassert(rEstMasRelevantes.dur_media - 747.6957692082626 < 0.001)\nassert(rEstMasRelevantes.dur_max == 35130)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}